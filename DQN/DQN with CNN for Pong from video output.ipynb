{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nature.com/articles/nature14236\n",
    "\n",
    "# Pong with DQN and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Trying to write to monitor directory ./PongVideos/ with existing monitor files: ./PongVideos/openaigym.manifest.0.12693.manifest.json.\n\n You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-38cdfa67edbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./PongVideos/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mepisode_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepisode_id\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, directory, video_callable, force, resume, write_upon_reset, uid, mode)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         self._start(directory, video_callable, force, resume,\n\u001b[0;32m---> 27\u001b[0;31m                             write_upon_reset, uid, mode)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, directory, video_callable, force, resume, write_upon_reset, uid, mode)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 raise error.Error('''Trying to write to monitor directory {} with existing monitor files: {}.\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m  You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.'''.format(directory, ', '.join(training_manifests[:5])))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor_closer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Trying to write to monitor directory ./PongVideos/ with existing monitor files: ./PongVideos/openaigym.manifest.0.12693.manifest.json.\n\n You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "@author: udemy\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "\n",
    "directory = './PongVideos/'\n",
    "env = gym.wrappers.Monitor(env, directory, video_callable=lambda episode_id: episode_id%20==0)\n",
    "\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 10\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 512\n",
    "\n",
    "replay_mem_size = 100000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 2000\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 18\n",
    "\n",
    "clip_error = True\n",
    "normalize_image = True\n",
    "\n",
    "file2save = 'pong_save.pth'\n",
    "save_model_frequency = 10000\n",
    "resume_previous_training = False\n",
    "\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon\n",
    "\n",
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)\n",
    "    \n",
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def plot_results():\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards_total, alpha=0.6, color='red')\n",
    "    plt.savefig(\"Pong-results.png\")\n",
    "    plt.close()\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer,1)\n",
    "\n",
    "        #self.activation = nn.Tanh()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if normalize_image:\n",
    "            x = x / 255\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        \n",
    "        output_conv = output_conv.view(output_conv.size(0), -1) # flatten\n",
    "        \n",
    "        output_advantage = self.advantage1(output_conv)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value1(output_conv)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "\n",
    "        return output_final\n",
    "    \n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print(\"Loading previously saved model ... \")\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = preprocess_frame(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                \n",
    "                action = torch.max(action_from_nn,1)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = [ preprocess_frame(frame) for frame in state ] \n",
    "        state = torch.cat(state)\n",
    "        \n",
    "        new_state = [ preprocess_frame(frame) for frame in new_state ] \n",
    "        new_state = torch.cat(new_state)\n",
    "\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "\n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        \n",
    "        \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        if self.number_of_frames % save_model_frequency == 0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.number_of_frames += 1\n",
    "        \n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "rewards_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            rewards_total.append(score)\n",
    "            \n",
    "            mean_reward_100 = sum(rewards_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode > 0):\n",
    "                \n",
    "                plot_results()\n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(rewards_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(rewards_total)/len(rewards_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "\n",
    "\n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
